BDEPEND=app-alternatives/ninja >=dev-build/cmake-3.20.5
DEFINED_PHASES=compile configure install prepare test
DEPEND=curl? ( net-misc/curl:= ) openblas? ( sci-libs/openblas:= ) blis? ( sci-libs/blis:= ) hip? ( >=dev-util/hip-6.3:= )
DESCRIPTION=Port of Facebook's LLaMA model in C/C++
EAPI=8
HOMEPAGE=https://github.com/ggerganov/llama.cpp
INHERIT=cmake rocm
IUSE=curl openblas blis hip +amdgpu_targets_gfx906 +amdgpu_targets_gfx908 +amdgpu_targets_gfx90a +amdgpu_targets_gfx942 +amdgpu_targets_gfx1030 +amdgpu_targets_gfx1100 amdgpu_targets_gfx803 amdgpu_targets_gfx900 amdgpu_targets_gfx940 amdgpu_targets_gfx941 amdgpu_targets_gfx1010 amdgpu_targets_gfx1011 amdgpu_targets_gfx1012 amdgpu_targets_gfx1031 amdgpu_targets_gfx1101 amdgpu_targets_gfx1102
KEYWORDS=~amd64
LICENSE=MIT
RDEPEND=curl? ( net-misc/curl:= ) openblas? ( sci-libs/openblas:= ) blis? ( sci-libs/blis:= ) hip? ( >=dev-util/hip-6.3:= ) dev-python/numpy
REQUIRED_USE=?? ( openblas blis )
SLOT=0
SRC_URI=https://github.com/ggerganov/llama.cpp/archive/refs/tags/b4576.tar.gz -> llama-cpp-0_pre4576.tar.gz
_eclasses_=toolchain-funcs	6afdb6107430c1832ca7e16aacbf8fa1	multilib	b2a329026f2e404e9e371097dda47f96	flag-o-matic	e8de74bac929ba17427e740e95707d00	multiprocessing	1e32df7deee68372153dca65f4a7c21f	ninja-utils	2df4e452cea39a9ec8fb543ce059f8d6	xdg-utils	42869b3c8d86a70ef3cf75165a395e09	cmake	c0c9c21d01b8a96d2d736c554daedc57	rocm	826765f795a41b937d1bfe8e709346cd
_md5_=1d9c0299b03b67d9071dd9f21be5ab7d
