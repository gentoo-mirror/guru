BDEPEND=app-alternatives/ninja >=dev-build/cmake-3.20.5
DEFINED_PHASES=compile configure install prepare setup test
DEPEND=curl? ( net-misc/curl:= ) openblas? ( sci-libs/openblas:= ) openmp? ( llvm-runtimes/openmp:= ) blis? ( sci-libs/blis:= ) hip? ( >=dev-util/hip-6.3:= >=sci-libs/hipBLAS-6.3:= ) cuda? ( dev-util/nvidia-cuda-toolkit:= ) opencl? ( dev-util/opencl-headers ) vulkan? ( dev-util/vulkan-headers )
DESCRIPTION=Port of Facebook's LLaMA model in C/C++
EAPI=8
HOMEPAGE=https://github.com/ggml-org/llama.cpp
INHERIT=cmake cuda rocm linux-info
IUSE=curl openblas +openmp blis hip cuda opencl vulkan +amdgpu_targets_gfx908 +amdgpu_targets_gfx90a +amdgpu_targets_gfx942 +amdgpu_targets_gfx1030 +amdgpu_targets_gfx1100 amdgpu_targets_gfx803 amdgpu_targets_gfx900 amdgpu_targets_gfx906 amdgpu_targets_gfx940 amdgpu_targets_gfx941 amdgpu_targets_gfx1010 amdgpu_targets_gfx1011 amdgpu_targets_gfx1012 amdgpu_targets_gfx1031 amdgpu_targets_gfx1101 amdgpu_targets_gfx1102 amdgpu_targets_gfx1200 amdgpu_targets_gfx1201
KEYWORDS=~amd64
LICENSE=MIT
RDEPEND=curl? ( net-misc/curl:= ) openblas? ( sci-libs/openblas:= ) openmp? ( llvm-runtimes/openmp:= ) blis? ( sci-libs/blis:= ) hip? ( >=dev-util/hip-6.3:= >=sci-libs/hipBLAS-6.3:= ) cuda? ( dev-util/nvidia-cuda-toolkit:= ) dev-python/numpy opencl? ( dev-libs/opencl-icd-loader ) vulkan? ( media-libs/vulkan-loader )
REQUIRED_USE=?? ( openblas blis )
SLOT=0
SRC_URI=https://github.com/ggml-org/llama.cpp/archive/refs/tags/b6318.tar.gz -> llama-cpp-0_pre6318.tar.gz
_eclasses_=toolchain-funcs	a0b29008c671a362b505f96fa80ce9c0	flag-o-matic	a7afe42e95fb46ce9691605acfb24672	multiprocessing	1e32df7deee68372153dca65f4a7c21f	ninja-utils	2df4e452cea39a9ec8fb543ce059f8d6	xdg-utils	42869b3c8d86a70ef3cf75165a395e09	cmake	460729dc36f68cf03b044bc1d367e34a	cuda	8b660e223a1695e3884ee4c7dc2c5059	rocm	049a642ed7dfce216d678c82044e33f9	linux-info	efd923656513c879204fec6638eadee5
_md5_=a92bfb72188390e11d55729209ccbf8e
